# -*- coding: utf-8 -*-
"""Word embedding Techniques.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1p8VHMAPY2v5PDUHFnHWVICxD4j44H_pv

### Word Embedding Techniques using Embedding Layer in Keras
"""

### Libraries USed Tensorflow> 2.0  and keras

!pip install tensorflow

import tensorflow as tf
print(tf.__version__)

##tensorflow >2.0
from tensorflow.keras.preprocessing.text import one_hot



### sentences - we are applying ohe on it and later convert this into vector using embedding
sent=[  'the glass of milk',
     'the glass of juice',
     'the cup of tea',
    'I am a good boy',
     'I am a good developer',
     'understand the meaning of words',
     'your videos are good']

sent

### Vocabulary size - dimensions; increases vocab size takes more time
voc_size=500

"""#### One Hot Representation"""

onehot_repr=[one_hot(words,voc_size)for words in sent] # here index corresponding to the words shows i.e. the value of 'the' at 25 index is 1
print(onehot_repr)

"""### Word Embedding Represntation"""

from tensorflow.keras.layers import Embedding
from tensorflow.keras.preprocessing.sequence import pad_sequences #used for pre and post padding
from tensorflow.keras.models import Sequential #for sequential NN

import numpy as np

## pre padding
sent_length=8
embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)
print(embedded_docs)

## 10 feature dimensions
dim=10

model=Sequential()
model.add(Embedding(voc_size, 10, input_shape=(sent_length,)))
model.compile('adam','mse')

model.summary()

##'the glass of milk',
embedded_docs[0]

model.predict(np.array([embedded_docs[0]]))
# or print(model.predict(embedded_docs)[0])

print(model.predict(embedded_docs))

