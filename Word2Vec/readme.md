# ğŸ§  Word2Vec with Google News (300-Dimension Embeddings)

Hello everyone, hope you're all doing great! ğŸŒ¼  
Understanding how machines capture meaning â€” one concept at a time.

---

## ğŸ“Œ Project Overview

This project explores **Word Embeddings** using the pretrained **Google News Word2Vec model**, which provides:

- 300-dimensional vectors
- ~100 billion words of training data
- Context-based semantic learning

The aim is to understand how words can be represented mathematically so that computers can analyze their relationships and meanings.

---

## ğŸš€ Features Implemented

- ğŸ”¹ Loaded Pretrained Word2Vec Model
  - Using `word2vec-google-news-300` from Gensim

- ğŸ”¹ Vector Extraction
  - Example: Getting 300D vector for the word `king`

- ğŸ”¹ Cosine Similarity
  - Measuring how close two words are in semantic meaning

- ğŸ”¹ Analogy Computation
  - Classic example: `king - man + woman â‰ˆ queen`

- ğŸ”¹ Semantic Neighbor Search
  - Finding similar words in embedding space

---

## ğŸ› ï¸ Tech Stack

- Python
- Gensim
- Google Colab
- Pretrained Word2Vec (Google News)

---

## ğŸ“‚ Project Workflow

1. Install & import Gensim
2. Load Google News Word2Vec Model
3. Fetch vector embeddings for a given word
4. Compute similarity scores
5. Perform analogy operations
6. Observe semantic relationships in word space

---

## ğŸ“Š Output

- 300-dimensional vector outputs
- Cosine similarity values
- Analogy predictions (ex: queen)
- Semantically similar word lists

---

## ğŸ¯ Learning Outcomes

- Understood how word embeddings represent meaning in vectors
- Learned semantic similarity measurements
- Observed how analogies can be solved mathematically
- Built intuition for how embeddings power modern NLP models

---

## ğŸ“Œ Future Enhancements

- Train own Word2Vec model on a custom dataset
- Compare Word2Vec with GloVe & FastText
- Build a text classifier using embeddings
- Move toward Transformer-based embeddings (BERT / GPT)

---

## ğŸ™Œ Author

**Shreya Gupta**
Aspiring AI/ML Engineer | NLP Enthusiast

---

âœ¨ Before BERT.  
âœ¨ Before GPT.  
âœ¨ Before Transformers.  

â¡ï¸ **Word Embeddings are where NLP truly begins.** ğŸš€

---

## â¤ï¸ Closing Note

Thank you for exploring this project!  
Suggestions and contributions are always welcome ğŸ˜Š

